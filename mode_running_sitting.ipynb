{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader , TensorDataset\n",
    "# from torch.utils.data import Dataset\n",
    "import matplotlib\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, device, label):\n",
    "    datas = []\n",
    "    with open(filename, 'r') as outfile:\n",
    "        json_data = json.load(outfile)\n",
    "        for frame in json_data:\n",
    "            frame_data = json.loads(frame)\n",
    "            data = []\n",
    "            for _, landmark_data in frame_data.items():\n",
    "                data.extend([\n",
    "                    landmark_data['x'],\n",
    "                    landmark_data['y'],\n",
    "                    landmark_data['z'],\n",
    "                    landmark_data['visibility']\n",
    "                ])\n",
    "            datas.append(data)\n",
    "\n",
    "    data_tensor = torch.tensor(datas, device=device, dtype=torch.float32)\n",
    "    labels = torch.full((data_tensor.shape[0], 1), label, device=device, dtype=torch.float32)\n",
    "    \n",
    "    return torch.cat((data_tensor, labels), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8962,  0.4425, -0.1782,  ...,  0.0566,  0.0543,  0.0000],\n",
       "        [ 0.5514,  0.2383, -0.3167,  ..., -0.2837,  0.4476,  1.0000],\n",
       "        [ 0.8681,  0.4934,  0.2408,  ..., -0.3232,  0.4910,  0.0000],\n",
       "        ...,\n",
       "        [ 0.4414,  0.1374, -0.5572,  ...,  0.1411,  0.8073,  1.0000],\n",
       "        [ 0.7084,  0.5419, -0.2331,  ...,  0.0695,  0.9801,  0.0000],\n",
       "        [ 0.2377,  0.1131, -0.4441,  ..., -0.0919,  0.8065,  1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data_0 = load_data('Database/Running and sitting/0/0.json', device,0)\n",
    "data_1 = load_data('Database/Running and sitting/1/1.json', device,1)\n",
    "label = {\"Sitting\",\"Running\"}\n",
    "data_0_tt = torch.cat((data_0,data_0))\n",
    "data_0 = torch.cat((data_0,data_0_tt))\n",
    "total_data = torch.cat((data_0,data_1))\n",
    "r = torch.randperm(total_data.size(0))\n",
    "c = torch.arange(total_data.size(1), dtype=torch.long)\n",
    "total_data = total_data[r][:, c]\n",
    "total_data\n",
    "\n",
    "\n",
    "# train_ratio = 0.8\n",
    "# train_size = int(train_ratio * total_data.size(0))\n",
    "# val_size = total_data.size(0) - train_size\n",
    "\n",
    "# train_data, val_data = torch.utils.data.random_split(total_data, [train_size, val_size])\n",
    "\n",
    "# train_data = torch.stack([total_data[i] for i in train_data.indices])\n",
    "# val_data = torch.stack([total_data[i] for i in val_data.indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = total_data[:, :-1].cpu().numpy()\n",
    "y = total_data[:, -1].cpu().numpy()\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_normalized, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert back to PyTorch tensors\n",
    "train_data = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "train_labels = torch.tensor(y_train, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "val_data = torch.tensor(X_val, dtype=torch.float32, device=device)\n",
    "val_labels = torch.tensor(y_val, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "test_data = torch.tensor(X_test, dtype=torch.float32, device=device)\n",
    "test_labels = torch.tensor(y_test, dtype=torch.float32, device=device).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Neural(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Neural, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_size = data_0.size(1)  \n",
    "model = Neural(input_size-1).to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.BCELoss()\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(val_data, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "#     size = len(dataloader)  \n",
    "#     model.train()\n",
    "#     for batch, data in enumerate(dataloader):\n",
    "#         X = data[:-1]\n",
    "#         y = data[-1].int()\n",
    "#         if (y == 0).item():\n",
    "#             y =  torch.tensor([0,1], dtype=torch.float32, device=device)\n",
    "#         else:\n",
    "#             y =  torch.tensor([1,0], dtype=torch.float32, device=device)\n",
    "#         pred = model(X)\n",
    "#         loss = loss_fn(pred, y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         if batch % 100 == 0:\n",
    "#             loss, current = loss.item(), batch * batch_size + len(X)\n",
    "#             print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "\n",
    "# def test_loop(dataloader, model, loss_fn):\n",
    "\n",
    "#     model.eval()\n",
    "#     size = len(dataloader)\n",
    "#     num_batches = len(dataloader)\n",
    "#     test_loss, correct = 0, 0\n",
    "#     # count = 0\n",
    "#     # count_0 = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for data in dataloader:\n",
    "#             X = data[:-1]\n",
    "#             y = data[-1].int()\n",
    "#             if (y == 0).item():\n",
    "#                 y =  torch.tensor([0,1], dtype=torch.float32, device=device)\n",
    "#             else:\n",
    "#                 y =  torch.tensor([1,0], dtype=torch.float32, device=device)\n",
    "\n",
    "#             pred = model(X)\n",
    "#             # if pred == 1:\n",
    "#             #     count_0+=1\n",
    "#             test_loss += loss_fn(pred, y).item()\n",
    "#             correct +=  ((pred > 0.5) == y).type(torch.float).sum().item()\n",
    "#     test_loss /= num_batches\n",
    "#     correct /= size\n",
    "#     print(f\"Test Error: \\n Accuracy: {(100*correct):>0.5f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "# for t in range(epochs):\n",
    "#     print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "#     train_loop(train_data, model, loss_fn, optimizer)\n",
    "#     test_loop(val_data, model, loss_fn)\n",
    "# print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train loss: 0.6880, Val loss: 0.6826, Val accuracy: 0.6702\n",
      "Epoch 2/200, Train loss: 0.6785, Val loss: 0.6745, Val accuracy: 0.7500\n",
      "Epoch 3/200, Train loss: 0.6695, Val loss: 0.6663, Val accuracy: 0.7660\n",
      "Epoch 4/200, Train loss: 0.6610, Val loss: 0.6578, Val accuracy: 0.7766\n",
      "Epoch 5/200, Train loss: 0.6526, Val loss: 0.6487, Val accuracy: 0.7766\n",
      "Epoch 6/200, Train loss: 0.6418, Val loss: 0.6392, Val accuracy: 0.7766\n",
      "Epoch 7/200, Train loss: 0.6320, Val loss: 0.6280, Val accuracy: 0.7819\n",
      "Epoch 8/200, Train loss: 0.6204, Val loss: 0.6160, Val accuracy: 0.7926\n",
      "Epoch 9/200, Train loss: 0.6087, Val loss: 0.6031, Val accuracy: 0.7872\n",
      "Epoch 10/200, Train loss: 0.5955, Val loss: 0.5898, Val accuracy: 0.7926\n",
      "Epoch 11/200, Train loss: 0.5820, Val loss: 0.5757, Val accuracy: 0.7979\n",
      "Epoch 12/200, Train loss: 0.5696, Val loss: 0.5615, Val accuracy: 0.7979\n",
      "Epoch 13/200, Train loss: 0.5567, Val loss: 0.5474, Val accuracy: 0.8138\n",
      "Epoch 14/200, Train loss: 0.5439, Val loss: 0.5335, Val accuracy: 0.8138\n",
      "Epoch 15/200, Train loss: 0.5317, Val loss: 0.5203, Val accuracy: 0.8191\n",
      "Epoch 16/200, Train loss: 0.5181, Val loss: 0.5076, Val accuracy: 0.8191\n",
      "Epoch 17/200, Train loss: 0.5077, Val loss: 0.4951, Val accuracy: 0.8298\n",
      "Epoch 18/200, Train loss: 0.4966, Val loss: 0.4842, Val accuracy: 0.8245\n",
      "Epoch 19/200, Train loss: 0.4866, Val loss: 0.4736, Val accuracy: 0.8298\n",
      "Epoch 20/200, Train loss: 0.4739, Val loss: 0.4643, Val accuracy: 0.8298\n",
      "Epoch 21/200, Train loss: 0.4678, Val loss: 0.4548, Val accuracy: 0.8351\n",
      "Epoch 22/200, Train loss: 0.4592, Val loss: 0.4463, Val accuracy: 0.8404\n",
      "Epoch 23/200, Train loss: 0.4517, Val loss: 0.4388, Val accuracy: 0.8404\n",
      "Epoch 24/200, Train loss: 0.4468, Val loss: 0.4316, Val accuracy: 0.8511\n",
      "Epoch 25/200, Train loss: 0.4386, Val loss: 0.4253, Val accuracy: 0.8511\n",
      "Epoch 26/200, Train loss: 0.4336, Val loss: 0.4196, Val accuracy: 0.8564\n",
      "Epoch 27/200, Train loss: 0.4309, Val loss: 0.4145, Val accuracy: 0.8564\n",
      "Epoch 28/200, Train loss: 0.4230, Val loss: 0.4094, Val accuracy: 0.8564\n",
      "Epoch 29/200, Train loss: 0.4196, Val loss: 0.4046, Val accuracy: 0.8511\n",
      "Epoch 30/200, Train loss: 0.4113, Val loss: 0.4007, Val accuracy: 0.8457\n",
      "Epoch 31/200, Train loss: 0.4090, Val loss: 0.3968, Val accuracy: 0.8457\n",
      "Epoch 32/200, Train loss: 0.4041, Val loss: 0.3931, Val accuracy: 0.8457\n",
      "Epoch 33/200, Train loss: 0.4013, Val loss: 0.3900, Val accuracy: 0.8457\n",
      "Epoch 34/200, Train loss: 0.3992, Val loss: 0.3867, Val accuracy: 0.8457\n",
      "Epoch 35/200, Train loss: 0.3927, Val loss: 0.3837, Val accuracy: 0.8457\n",
      "Epoch 36/200, Train loss: 0.3904, Val loss: 0.3808, Val accuracy: 0.8457\n",
      "Epoch 37/200, Train loss: 0.3863, Val loss: 0.3781, Val accuracy: 0.8511\n",
      "Epoch 38/200, Train loss: 0.3815, Val loss: 0.3757, Val accuracy: 0.8564\n",
      "Epoch 39/200, Train loss: 0.3781, Val loss: 0.3735, Val accuracy: 0.8617\n",
      "Epoch 40/200, Train loss: 0.3787, Val loss: 0.3709, Val accuracy: 0.8617\n",
      "Epoch 41/200, Train loss: 0.3734, Val loss: 0.3692, Val accuracy: 0.8617\n",
      "Epoch 42/200, Train loss: 0.3710, Val loss: 0.3670, Val accuracy: 0.8617\n",
      "Epoch 43/200, Train loss: 0.3674, Val loss: 0.3649, Val accuracy: 0.8670\n",
      "Epoch 44/200, Train loss: 0.3656, Val loss: 0.3634, Val accuracy: 0.8617\n",
      "Epoch 45/200, Train loss: 0.3626, Val loss: 0.3618, Val accuracy: 0.8564\n",
      "Epoch 46/200, Train loss: 0.3603, Val loss: 0.3597, Val accuracy: 0.8670\n",
      "Epoch 47/200, Train loss: 0.3592, Val loss: 0.3581, Val accuracy: 0.8670\n",
      "Epoch 48/200, Train loss: 0.3553, Val loss: 0.3569, Val accuracy: 0.8670\n",
      "Epoch 49/200, Train loss: 0.3535, Val loss: 0.3553, Val accuracy: 0.8670\n",
      "Epoch 50/200, Train loss: 0.3509, Val loss: 0.3539, Val accuracy: 0.8670\n",
      "Epoch 51/200, Train loss: 0.3489, Val loss: 0.3525, Val accuracy: 0.8670\n",
      "Epoch 52/200, Train loss: 0.3451, Val loss: 0.3509, Val accuracy: 0.8670\n",
      "Epoch 53/200, Train loss: 0.3419, Val loss: 0.3494, Val accuracy: 0.8670\n",
      "Epoch 54/200, Train loss: 0.3412, Val loss: 0.3487, Val accuracy: 0.8723\n",
      "Epoch 55/200, Train loss: 0.3405, Val loss: 0.3475, Val accuracy: 0.8723\n",
      "Epoch 56/200, Train loss: 0.3384, Val loss: 0.3463, Val accuracy: 0.8723\n",
      "Epoch 57/200, Train loss: 0.3351, Val loss: 0.3452, Val accuracy: 0.8723\n",
      "Epoch 58/200, Train loss: 0.3320, Val loss: 0.3443, Val accuracy: 0.8777\n",
      "Epoch 59/200, Train loss: 0.3317, Val loss: 0.3431, Val accuracy: 0.8777\n",
      "Epoch 60/200, Train loss: 0.3283, Val loss: 0.3420, Val accuracy: 0.8777\n",
      "Epoch 61/200, Train loss: 0.3259, Val loss: 0.3412, Val accuracy: 0.8777\n",
      "Epoch 62/200, Train loss: 0.3229, Val loss: 0.3405, Val accuracy: 0.8777\n",
      "Epoch 63/200, Train loss: 0.3228, Val loss: 0.3395, Val accuracy: 0.8723\n",
      "Epoch 64/200, Train loss: 0.3252, Val loss: 0.3383, Val accuracy: 0.8777\n",
      "Epoch 65/200, Train loss: 0.3184, Val loss: 0.3381, Val accuracy: 0.8670\n",
      "Epoch 66/200, Train loss: 0.3203, Val loss: 0.3373, Val accuracy: 0.8670\n",
      "Epoch 67/200, Train loss: 0.3161, Val loss: 0.3370, Val accuracy: 0.8723\n",
      "Epoch 68/200, Train loss: 0.3135, Val loss: 0.3356, Val accuracy: 0.8723\n",
      "Epoch 69/200, Train loss: 0.3109, Val loss: 0.3349, Val accuracy: 0.8723\n",
      "Epoch 70/200, Train loss: 0.3104, Val loss: 0.3344, Val accuracy: 0.8723\n",
      "Epoch 71/200, Train loss: 0.3117, Val loss: 0.3331, Val accuracy: 0.8723\n",
      "Epoch 72/200, Train loss: 0.3097, Val loss: 0.3327, Val accuracy: 0.8723\n",
      "Epoch 73/200, Train loss: 0.3040, Val loss: 0.3320, Val accuracy: 0.8723\n",
      "Epoch 74/200, Train loss: 0.3051, Val loss: 0.3314, Val accuracy: 0.8670\n",
      "Epoch 75/200, Train loss: 0.3038, Val loss: 0.3308, Val accuracy: 0.8670\n",
      "Epoch 76/200, Train loss: 0.2991, Val loss: 0.3295, Val accuracy: 0.8670\n",
      "Epoch 77/200, Train loss: 0.3007, Val loss: 0.3291, Val accuracy: 0.8670\n",
      "Epoch 78/200, Train loss: 0.2995, Val loss: 0.3283, Val accuracy: 0.8670\n",
      "Epoch 79/200, Train loss: 0.2987, Val loss: 0.3276, Val accuracy: 0.8670\n",
      "Epoch 80/200, Train loss: 0.2940, Val loss: 0.3273, Val accuracy: 0.8777\n",
      "Epoch 81/200, Train loss: 0.2914, Val loss: 0.3271, Val accuracy: 0.8777\n",
      "Epoch 82/200, Train loss: 0.2923, Val loss: 0.3257, Val accuracy: 0.8777\n",
      "Epoch 83/200, Train loss: 0.2900, Val loss: 0.3254, Val accuracy: 0.8777\n",
      "Epoch 84/200, Train loss: 0.2880, Val loss: 0.3256, Val accuracy: 0.8777\n",
      "Epoch 85/200, Train loss: 0.2868, Val loss: 0.3239, Val accuracy: 0.8777\n",
      "Epoch 86/200, Train loss: 0.2852, Val loss: 0.3238, Val accuracy: 0.8777\n",
      "Epoch 87/200, Train loss: 0.2852, Val loss: 0.3233, Val accuracy: 0.8777\n",
      "Epoch 88/200, Train loss: 0.2837, Val loss: 0.3227, Val accuracy: 0.8777\n",
      "Epoch 89/200, Train loss: 0.2794, Val loss: 0.3222, Val accuracy: 0.8777\n",
      "Epoch 90/200, Train loss: 0.2821, Val loss: 0.3214, Val accuracy: 0.8777\n",
      "Epoch 91/200, Train loss: 0.2765, Val loss: 0.3209, Val accuracy: 0.8777\n",
      "Epoch 92/200, Train loss: 0.2746, Val loss: 0.3197, Val accuracy: 0.8830\n",
      "Epoch 93/200, Train loss: 0.2793, Val loss: 0.3191, Val accuracy: 0.8883\n",
      "Epoch 94/200, Train loss: 0.2775, Val loss: 0.3191, Val accuracy: 0.8883\n",
      "Epoch 95/200, Train loss: 0.2723, Val loss: 0.3190, Val accuracy: 0.8883\n",
      "Epoch 96/200, Train loss: 0.2711, Val loss: 0.3183, Val accuracy: 0.8883\n",
      "Epoch 97/200, Train loss: 0.2672, Val loss: 0.3170, Val accuracy: 0.8883\n",
      "Epoch 98/200, Train loss: 0.2684, Val loss: 0.3170, Val accuracy: 0.8883\n",
      "Epoch 99/200, Train loss: 0.2698, Val loss: 0.3160, Val accuracy: 0.8936\n",
      "Epoch 100/200, Train loss: 0.2653, Val loss: 0.3150, Val accuracy: 0.8989\n",
      "Epoch 101/200, Train loss: 0.2623, Val loss: 0.3151, Val accuracy: 0.8989\n",
      "Epoch 102/200, Train loss: 0.2633, Val loss: 0.3143, Val accuracy: 0.8989\n",
      "Epoch 103/200, Train loss: 0.2606, Val loss: 0.3132, Val accuracy: 0.8989\n",
      "Epoch 104/200, Train loss: 0.2597, Val loss: 0.3133, Val accuracy: 0.8989\n",
      "Epoch 105/200, Train loss: 0.2589, Val loss: 0.3124, Val accuracy: 0.8989\n",
      "Epoch 106/200, Train loss: 0.2583, Val loss: 0.3118, Val accuracy: 0.8989\n",
      "Epoch 107/200, Train loss: 0.2601, Val loss: 0.3102, Val accuracy: 0.8989\n",
      "Epoch 108/200, Train loss: 0.2555, Val loss: 0.3104, Val accuracy: 0.8989\n",
      "Epoch 109/200, Train loss: 0.2518, Val loss: 0.3103, Val accuracy: 0.8989\n",
      "Epoch 110/200, Train loss: 0.2512, Val loss: 0.3099, Val accuracy: 0.8936\n",
      "Epoch 111/200, Train loss: 0.2493, Val loss: 0.3085, Val accuracy: 0.8989\n",
      "Epoch 112/200, Train loss: 0.2493, Val loss: 0.3086, Val accuracy: 0.8936\n",
      "Epoch 113/200, Train loss: 0.2473, Val loss: 0.3080, Val accuracy: 0.8936\n",
      "Epoch 114/200, Train loss: 0.2471, Val loss: 0.3076, Val accuracy: 0.8936\n",
      "Epoch 115/200, Train loss: 0.2461, Val loss: 0.3066, Val accuracy: 0.8936\n",
      "Epoch 116/200, Train loss: 0.2432, Val loss: 0.3069, Val accuracy: 0.8936\n",
      "Epoch 117/200, Train loss: 0.2434, Val loss: 0.3064, Val accuracy: 0.8936\n",
      "Epoch 118/200, Train loss: 0.2410, Val loss: 0.3057, Val accuracy: 0.8936\n",
      "Epoch 119/200, Train loss: 0.2397, Val loss: 0.3047, Val accuracy: 0.8936\n",
      "Epoch 120/200, Train loss: 0.2362, Val loss: 0.3051, Val accuracy: 0.8936\n",
      "Epoch 121/200, Train loss: 0.2381, Val loss: 0.3050, Val accuracy: 0.8936\n",
      "Epoch 122/200, Train loss: 0.2348, Val loss: 0.3041, Val accuracy: 0.8936\n",
      "Epoch 123/200, Train loss: 0.2335, Val loss: 0.3038, Val accuracy: 0.8936\n",
      "Epoch 124/200, Train loss: 0.2328, Val loss: 0.3044, Val accuracy: 0.8936\n",
      "Epoch 125/200, Train loss: 0.2320, Val loss: 0.3038, Val accuracy: 0.8936\n",
      "Epoch 126/200, Train loss: 0.2301, Val loss: 0.3028, Val accuracy: 0.8936\n",
      "Epoch 127/200, Train loss: 0.2313, Val loss: 0.3031, Val accuracy: 0.8936\n",
      "Epoch 128/200, Train loss: 0.2296, Val loss: 0.3026, Val accuracy: 0.8936\n",
      "Epoch 129/200, Train loss: 0.2248, Val loss: 0.3019, Val accuracy: 0.8936\n",
      "Epoch 130/200, Train loss: 0.2273, Val loss: 0.3011, Val accuracy: 0.8936\n",
      "Epoch 131/200, Train loss: 0.2250, Val loss: 0.3007, Val accuracy: 0.8936\n",
      "Epoch 132/200, Train loss: 0.2231, Val loss: 0.2995, Val accuracy: 0.8936\n",
      "Epoch 133/200, Train loss: 0.2220, Val loss: 0.2994, Val accuracy: 0.8936\n",
      "Epoch 134/200, Train loss: 0.2194, Val loss: 0.2988, Val accuracy: 0.8936\n",
      "Epoch 135/200, Train loss: 0.2194, Val loss: 0.2985, Val accuracy: 0.8936\n",
      "Epoch 136/200, Train loss: 0.2188, Val loss: 0.2988, Val accuracy: 0.8936\n",
      "Epoch 137/200, Train loss: 0.2180, Val loss: 0.2979, Val accuracy: 0.8936\n",
      "Epoch 138/200, Train loss: 0.2148, Val loss: 0.2980, Val accuracy: 0.8936\n",
      "Epoch 139/200, Train loss: 0.2172, Val loss: 0.2965, Val accuracy: 0.8936\n",
      "Epoch 140/200, Train loss: 0.2132, Val loss: 0.2966, Val accuracy: 0.8936\n",
      "Epoch 141/200, Train loss: 0.2108, Val loss: 0.2955, Val accuracy: 0.8936\n",
      "Epoch 142/200, Train loss: 0.2111, Val loss: 0.2970, Val accuracy: 0.8936\n",
      "Epoch 143/200, Train loss: 0.2119, Val loss: 0.2969, Val accuracy: 0.8936\n",
      "Epoch 144/200, Train loss: 0.2094, Val loss: 0.2952, Val accuracy: 0.8936\n",
      "Epoch 145/200, Train loss: 0.2109, Val loss: 0.2954, Val accuracy: 0.8936\n",
      "Epoch 146/200, Train loss: 0.2068, Val loss: 0.2945, Val accuracy: 0.8936\n",
      "Epoch 147/200, Train loss: 0.2077, Val loss: 0.2937, Val accuracy: 0.8936\n",
      "Epoch 148/200, Train loss: 0.2051, Val loss: 0.2940, Val accuracy: 0.8936\n",
      "Epoch 149/200, Train loss: 0.2050, Val loss: 0.2940, Val accuracy: 0.8936\n",
      "Epoch 150/200, Train loss: 0.2050, Val loss: 0.2940, Val accuracy: 0.8936\n",
      "Epoch 151/200, Train loss: 0.2046, Val loss: 0.2916, Val accuracy: 0.8936\n",
      "Epoch 152/200, Train loss: 0.2027, Val loss: 0.2941, Val accuracy: 0.8936\n",
      "Epoch 153/200, Train loss: 0.2023, Val loss: 0.2915, Val accuracy: 0.8936\n",
      "Epoch 154/200, Train loss: 0.1993, Val loss: 0.2925, Val accuracy: 0.8936\n",
      "Epoch 155/200, Train loss: 0.1980, Val loss: 0.2911, Val accuracy: 0.8936\n",
      "Epoch 156/200, Train loss: 0.1971, Val loss: 0.2908, Val accuracy: 0.8936\n",
      "Epoch 157/200, Train loss: 0.1967, Val loss: 0.2911, Val accuracy: 0.8936\n",
      "Epoch 158/200, Train loss: 0.1964, Val loss: 0.2892, Val accuracy: 0.8936\n",
      "Epoch 159/200, Train loss: 0.1942, Val loss: 0.2918, Val accuracy: 0.8936\n",
      "Epoch 160/200, Train loss: 0.1923, Val loss: 0.2913, Val accuracy: 0.8936\n",
      "Epoch 161/200, Train loss: 0.1942, Val loss: 0.2905, Val accuracy: 0.8936\n",
      "Epoch 162/200, Train loss: 0.1921, Val loss: 0.2904, Val accuracy: 0.8936\n",
      "Epoch 163/200, Train loss: 0.1892, Val loss: 0.2892, Val accuracy: 0.8936\n",
      "Epoch 164/200, Train loss: 0.1892, Val loss: 0.2895, Val accuracy: 0.8936\n",
      "Epoch 165/200, Train loss: 0.1877, Val loss: 0.2884, Val accuracy: 0.8936\n",
      "Epoch 166/200, Train loss: 0.1874, Val loss: 0.2890, Val accuracy: 0.8936\n",
      "Epoch 167/200, Train loss: 0.1846, Val loss: 0.2881, Val accuracy: 0.8936\n",
      "Epoch 168/200, Train loss: 0.1857, Val loss: 0.2886, Val accuracy: 0.8989\n",
      "Epoch 169/200, Train loss: 0.1845, Val loss: 0.2879, Val accuracy: 0.8989\n",
      "Epoch 170/200, Train loss: 0.1850, Val loss: 0.2880, Val accuracy: 0.8989\n",
      "Epoch 171/200, Train loss: 0.1813, Val loss: 0.2874, Val accuracy: 0.8989\n",
      "Epoch 172/200, Train loss: 0.1817, Val loss: 0.2861, Val accuracy: 0.8989\n",
      "Epoch 173/200, Train loss: 0.1796, Val loss: 0.2874, Val accuracy: 0.8989\n",
      "Epoch 174/200, Train loss: 0.1809, Val loss: 0.2870, Val accuracy: 0.8989\n",
      "Epoch 175/200, Train loss: 0.1772, Val loss: 0.2888, Val accuracy: 0.8936\n",
      "Epoch 176/200, Train loss: 0.1777, Val loss: 0.2884, Val accuracy: 0.8936\n",
      "Epoch 177/200, Train loss: 0.1784, Val loss: 0.2862, Val accuracy: 0.8989\n",
      "Epoch 178/200, Train loss: 0.1773, Val loss: 0.2865, Val accuracy: 0.8989\n",
      "Epoch 179/200, Train loss: 0.1753, Val loss: 0.2878, Val accuracy: 0.8989\n",
      "Epoch 180/200, Train loss: 0.1736, Val loss: 0.2864, Val accuracy: 0.8989\n",
      "Epoch 181/200, Train loss: 0.1707, Val loss: 0.2862, Val accuracy: 0.8883\n",
      "Epoch 182/200, Train loss: 0.1704, Val loss: 0.2856, Val accuracy: 0.8936\n",
      "Epoch 183/200, Train loss: 0.1695, Val loss: 0.2848, Val accuracy: 0.8989\n",
      "Epoch 184/200, Train loss: 0.1685, Val loss: 0.2849, Val accuracy: 0.8883\n",
      "Epoch 185/200, Train loss: 0.1681, Val loss: 0.2853, Val accuracy: 0.8936\n",
      "Epoch 186/200, Train loss: 0.1698, Val loss: 0.2864, Val accuracy: 0.8989\n",
      "Epoch 187/200, Train loss: 0.1670, Val loss: 0.2852, Val accuracy: 0.8936\n",
      "Epoch 188/200, Train loss: 0.1668, Val loss: 0.2844, Val accuracy: 0.8936\n",
      "Epoch 189/200, Train loss: 0.1658, Val loss: 0.2834, Val accuracy: 0.8936\n",
      "Epoch 190/200, Train loss: 0.1652, Val loss: 0.2836, Val accuracy: 0.8936\n",
      "Epoch 191/200, Train loss: 0.1632, Val loss: 0.2850, Val accuracy: 0.8989\n",
      "Epoch 192/200, Train loss: 0.1625, Val loss: 0.2850, Val accuracy: 0.8883\n",
      "Epoch 193/200, Train loss: 0.1610, Val loss: 0.2847, Val accuracy: 0.8936\n",
      "Epoch 194/200, Train loss: 0.1609, Val loss: 0.2855, Val accuracy: 0.8989\n",
      "Epoch 195/200, Train loss: 0.1602, Val loss: 0.2847, Val accuracy: 0.8936\n",
      "Epoch 196/200, Train loss: 0.1598, Val loss: 0.2827, Val accuracy: 0.8936\n",
      "Epoch 197/200, Train loss: 0.1595, Val loss: 0.2826, Val accuracy: 0.8989\n",
      "Epoch 198/200, Train loss: 0.1565, Val loss: 0.2839, Val accuracy: 0.8989\n",
      "Epoch 199/200, Train loss: 0.1566, Val loss: 0.2836, Val accuracy: 0.8936\n",
      "Epoch 200/200, Train loss: 0.1570, Val loss: 0.2813, Val accuracy: 0.8936\n",
      "Test loss: 0.3255, Test accuracy: 0.8723\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, y in dataloader:\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += ((pred > 0.5) == y).float().sum().item()\n",
    "    test_loss /= len(dataloader)\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return test_loss, accuracy\n",
    "\n",
    "for t in range(epochs):\n",
    "    train_loss = train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    val_loss, val_accuracy = test_loop(val_loader, model, loss_fn)\n",
    "    print(f\"Epoch {t+1}/{epochs}, Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Val accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "test_loss, test_accuracy = test_loop(test_loader, model, loss_fn)\n",
    "print(f\"Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_preds = model(val_data).cpu().numpy().flatten()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(val_preds, bins=50, edgecolor='black')\n",
    "plt.title('Distribution of Model Predictions')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0: Prediction = 0.4608137905597687\n",
      "Frame 1: Prediction = 0.4613989591598511\n",
      "Frame 2: Prediction = 0.461509644985199\n",
      "Frame 3: Prediction = 0.4613552689552307\n",
      "Frame 4: Prediction = 0.46139952540397644\n",
      "Frame 5: Prediction = 0.46159228682518005\n",
      "Frame 6: Prediction = 0.4616834819316864\n",
      "Frame 7: Prediction = 0.4618320167064667\n",
      "Frame 8: Prediction = 0.4617339074611664\n",
      "Frame 9: Prediction = 0.4614415764808655\n",
      "Frame 10: Prediction = 0.4615851640701294\n",
      "Frame 11: Prediction = 0.4614320695400238\n",
      "Frame 12: Prediction = 0.4616248607635498\n",
      "Frame 13: Prediction = 0.4615289568901062\n",
      "Frame 14: Prediction = 0.4615665376186371\n",
      "Frame 15: Prediction = 0.4619067907333374\n",
      "Frame 16: Prediction = 0.461957722902298\n",
      "Frame 17: Prediction = 0.46194422245025635\n",
      "Frame 18: Prediction = 0.46203216910362244\n",
      "Frame 19: Prediction = 0.4620305299758911\n",
      "Frame 20: Prediction = 0.4619731903076172\n",
      "Frame 21: Prediction = 0.46190476417541504\n",
      "Frame 22: Prediction = 0.4619828164577484\n",
      "Frame 23: Prediction = 0.46225911378860474\n",
      "Frame 24: Prediction = 0.4623558819293976\n",
      "Frame 25: Prediction = 0.4623524248600006\n",
      "Frame 26: Prediction = 0.4620789885520935\n",
      "Frame 27: Prediction = 0.46208277344703674\n",
      "Frame 28: Prediction = 0.4621988534927368\n",
      "Frame 29: Prediction = 0.4623415172100067\n",
      "Frame 30: Prediction = 0.4627785384654999\n",
      "Frame 31: Prediction = 0.46268436312675476\n",
      "Frame 32: Prediction = 0.4626351594924927\n",
      "Frame 33: Prediction = 0.46214616298675537\n",
      "Frame 34: Prediction = 0.4622768461704254\n",
      "Frame 35: Prediction = 0.4622216820716858\n",
      "Frame 36: Prediction = 0.46229228377342224\n",
      "Frame 37: Prediction = 0.4628167748451233\n",
      "Frame 38: Prediction = 0.46326878666877747\n",
      "Frame 39: Prediction = 0.4626426100730896\n",
      "Frame 40: Prediction = 0.46191874146461487\n",
      "Frame 41: Prediction = 0.4627276360988617\n",
      "Frame 42: Prediction = 0.4632749855518341\n",
      "Frame 43: Prediction = 0.4636286795139313\n",
      "Frame 44: Prediction = 0.463727205991745\n",
      "Frame 45: Prediction = 0.4638262689113617\n",
      "Frame 46: Prediction = 0.4635922908782959\n",
      "Frame 47: Prediction = 0.4638741910457611\n",
      "Frame 48: Prediction = 0.4631956219673157\n",
      "Frame 49: Prediction = 0.46325117349624634\n",
      "Frame 50: Prediction = 0.46311041712760925\n",
      "Frame 51: Prediction = 0.4633798599243164\n",
      "Frame 52: Prediction = 0.4634190499782562\n",
      "Frame 53: Prediction = 0.4633362293243408\n",
      "Frame 54: Prediction = 0.4631149172782898\n",
      "Frame 55: Prediction = 0.4627299904823303\n",
      "Frame 56: Prediction = 0.46296727657318115\n",
      "Frame 57: Prediction = 0.463043749332428\n",
      "Frame 58: Prediction = 0.46335047483444214\n",
      "Frame 59: Prediction = 0.46362221240997314\n",
      "Frame 60: Prediction = 0.46358194947242737\n",
      "Frame 61: Prediction = 0.46366086602211\n",
      "Frame 62: Prediction = 0.46361812949180603\n",
      "Frame 63: Prediction = 0.46388015151023865\n",
      "Frame 64: Prediction = 0.46381333470344543\n",
      "Frame 65: Prediction = 0.4638047218322754\n",
      "Frame 66: Prediction = 0.46430090069770813\n",
      "Frame 67: Prediction = 0.4645787179470062\n",
      "Frame 68: Prediction = 0.46452561020851135\n",
      "Frame 69: Prediction = 0.46446794271469116\n",
      "Frame 70: Prediction = 0.4641454815864563\n",
      "Frame 71: Prediction = 0.46410834789276123\n",
      "Frame 72: Prediction = 0.46422868967056274\n",
      "Frame 73: Prediction = 0.4638389050960541\n",
      "Frame 74: Prediction = 0.46385806798934937\n",
      "Frame 75: Prediction = 0.4644658863544464\n",
      "Frame 76: Prediction = 0.4655263423919678\n",
      "Frame 77: Prediction = 0.4636564552783966\n",
      "Frame 78: Prediction = 0.46396028995513916\n",
      "Frame 79: Prediction = 0.46424806118011475\n",
      "Frame 80: Prediction = 0.46469128131866455\n",
      "Frame 81: Prediction = 0.4641318619251251\n",
      "Frame 82: Prediction = 0.46460434794425964\n",
      "Frame 83: Prediction = 0.4646894931793213\n",
      "Frame 84: Prediction = 0.46495702862739563\n",
      "Frame 85: Prediction = 0.4660002887248993\n",
      "Frame 86: Prediction = 0.46657073497772217\n",
      "Frame 87: Prediction = 0.46620869636535645\n",
      "Frame 88: Prediction = 0.4662328064441681\n",
      "Frame 89: Prediction = 0.4663069248199463\n",
      "Frame 90: Prediction = 0.4651390612125397\n",
      "Frame 91: Prediction = 0.46501365303993225\n",
      "Frame 92: Prediction = 0.46526846289634705\n",
      "Frame 93: Prediction = 0.46552011370658875\n",
      "Frame 94: Prediction = 0.46619871258735657\n",
      "Frame 95: Prediction = 0.4664825201034546\n",
      "Frame 96: Prediction = 0.46740421652793884\n",
      "Frame 97: Prediction = 0.46666285395622253\n",
      "Frame 98: Prediction = 0.466907262802124\n",
      "Frame 99: Prediction = 0.46725642681121826\n",
      "Frame 100: Prediction = 0.46634989976882935\n",
      "Frame 101: Prediction = 0.466482013463974\n",
      "Frame 102: Prediction = 0.4667951762676239\n",
      "Frame 103: Prediction = 0.46685898303985596\n",
      "Frame 104: Prediction = 0.4664542078971863\n",
      "Frame 105: Prediction = 0.4660225808620453\n",
      "Frame 106: Prediction = 0.4659058153629303\n",
      "Frame 107: Prediction = 0.4658268392086029\n",
      "Frame 108: Prediction = 0.4658534526824951\n",
      "Frame 109: Prediction = 0.4656600058078766\n",
      "Frame 110: Prediction = 0.46609628200531006\n",
      "Frame 111: Prediction = 0.46593397855758667\n",
      "Frame 112: Prediction = 0.466493159532547\n",
      "Frame 113: Prediction = 0.46554216742515564\n",
      "Frame 114: Prediction = 0.4657689034938812\n",
      "Frame 115: Prediction = 0.4652928113937378\n",
      "Frame 116: Prediction = 0.4654235541820526\n",
      "Frame 117: Prediction = 0.46627795696258545\n",
      "Frame 118: Prediction = 0.46634531021118164\n",
      "Frame 119: Prediction = 0.4671129584312439\n",
      "Frame 120: Prediction = 0.46602579951286316\n",
      "Frame 121: Prediction = 0.46588778495788574\n",
      "Frame 122: Prediction = 0.4656224846839905\n",
      "Frame 123: Prediction = 0.46560418605804443\n",
      "Frame 124: Prediction = 0.4660070538520813\n",
      "Frame 125: Prediction = 0.46597951650619507\n",
      "Frame 126: Prediction = 0.46628811955451965\n",
      "Frame 127: Prediction = 0.46593159437179565\n",
      "Frame 128: Prediction = 0.4659039080142975\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     32\u001b[0m frame_rgb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 34\u001b[0m result \u001b[38;5;241m=\u001b[39m pose\u001b[38;5;241m.\u001b[39mprocess(frame_rgb)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mpose_landmarks:\n\u001b[0;32m     37\u001b[0m     dic \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Home\\anaconda3\\Lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001b[0m, in \u001b[0;36mPose.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    165\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m         \"enable_segmentation\" is set to true.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mprocess(input_data\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m: image})\n\u001b[0;32m    186\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mpose_landmarks\u001b[38;5;241m.\u001b[39mlandmark:  \u001b[38;5;66;03m# pytype: disable=attribute-error\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Home\\anaconda3\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:372\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    366\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    368\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    369\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    370\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mwait_until_idle()\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def load_data(json_data, device):\n",
    "    try:\n",
    "        frame_data = json.loads(json_data)\n",
    "        data = []\n",
    "        for _, landmark_data in frame_data.items():\n",
    "            data.extend([\n",
    "                landmark_data['x'],\n",
    "                landmark_data['y'],\n",
    "                landmark_data['z'],\n",
    "                landmark_data['visibility']\n",
    "            ])\n",
    "        data_tensor = torch.tensor([data], device=device, dtype=torch.float32)\n",
    "        return data_tensor\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error: {e}\")\n",
    "        print(f\"Problematic JSON data: {json_data}\")\n",
    "        return None\n",
    "\n",
    "video_path = './vid/usain.mp4'\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_number = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    result = pose.process(frame_rgb)\n",
    "\n",
    "    if result.pose_landmarks:\n",
    "        dic = {}\n",
    "        for mark, data_point in zip(mp_pose.PoseLandmark, result.pose_landmarks.landmark):\n",
    "            dic[mark.value] = {\n",
    "                \"landmark\": mark.name,\n",
    "                \"x\": data_point.x,\n",
    "                \"y\": data_point.y,\n",
    "                \"z\": data_point.z,\n",
    "                \"visibility\": data_point.visibility\n",
    "            }\n",
    "        json_object = json.dumps(dic)\n",
    "        \n",
    "        tensor_data = load_data(json_object, device=device)\n",
    "        if tensor_data is not None:\n",
    "            y = model(tensor_data)\n",
    "            print(f\"Frame {frame_number}: Prediction = {y.item()}\")\n",
    "            if y.item() > 0.5:\n",
    "                text = \"Running\"\n",
    "            else:\n",
    "                text = \"Sitting\"\n",
    "        \n",
    "        mp_drawing.draw_landmarks(frame, result.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    cv2.putText(frame, text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0),  2)\n",
    "\n",
    "    cv2.imshow('MediaPipe Pose', frame)\n",
    "\n",
    "    frame_number += 1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb_writer = SummaryWriter()\n",
    "# best_vloss = 1_000_000.\n",
    "\n",
    "\n",
    "# for epoch in range(2):  \n",
    "#     model.train(True)\n",
    "#     running_vloss = 0.0\n",
    "#     last_loss = train(epoch, tb_writer)\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for i , v_data in enumerate(val_data):\n",
    "#             label = v_data[-1]\n",
    "#             if label.item() == 1:\n",
    "#                 label =  torch.tensor([1], dtype=torch.float32, device=device)\n",
    "#             else:\n",
    "#                 label = torch.tensor([0], dtype=torch.float32, device=device)\n",
    "#             v_data = v_data[:-1]\n",
    "#             vout = model(v_data)\n",
    "#             vloss= loss_fn(vout,label)\n",
    "#             running_vloss += vloss\n",
    "#     avg_vloss = running_vloss / (i + 1)\n",
    "#     print('LOSS train {} valid {}'.format(last_loss, avg_vloss))\n",
    "\n",
    "#     print(f'Epoch {epoch+1} completed with loss: {last_loss}')\n",
    "\n",
    "# tb_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./models/RunningSitting_v3_better.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
